Comparative study of BERT Hybrid models

This repository contains BERT hybrid models - BERT + NN, BERT + Attention, BERT + CNN

Experiements 
1. the models on GLUE datasets - MRPC, WNLI, RTE, COLA.
2. the models are trained on BERT hybrid models - BERT + NN, BERT + Attention, BERT + CNN.
3. the models are also trained using Masked Language Modeling (MLM) tasks.
 
Result:
Increased ~8\% average evaluation metric (f1 for MRPC and accuracy for other datasets) value on 4 GLUE datasets. 